\section{tf.train}
提供了训练模型的类和函数。
\subsection{优化器}
优化器类提供方法计算损失函数对于变量的的梯度的计算方法，子类集合实现了像A
dagrad和GradientDescent等经典算法。
\subsubsection{Optimizer}
基础的优化类，定义了增加一个操作到训练模型的API，你不直接需要这个类而是需要它的一些像GradientDescentOptimizer, AdagradOptimizer,或者MomentumOptimizer的子类。
\textbf{用法}\newline
\begin{python}
# Create an optimizer with the desired parameters.
opt = GradientDescentOptimizer(learning_rate=0.1)
# Add Ops to the graph to minimize a cost by updating a list of variables.
# "cost" is a Tensor, and the list of variables contains tf.Variable
# objects.
opt_op = opt.minimize(cost, var_list=<list of variables>)
\end{python}
在训练程序的过程中你需要返回操作。
\begin{python}
# Execute opt_op to do one step of training:
opt_op.run()
\end{python}
\textbf{在应用他们之前处理梯度}\newline
条用minimize()计算题度，应用它们在变量上。如果你想在应用他们之前处理你可以按照下面的步骤使用优化器。
\begin{enumerate}
	\item 用comput\_gradients()计算梯度。
	\item 按照你的希望处理梯度。
	\item 用apply\_gradients()处理梯度。
\end{enumerate}
\begin{python}
# Create an optimizer.
opt = GradientDescentOptimizer(learning_rate=0.1)

# Compute the gradients for a list of variables.
grads_and_vars = opt.compute_gradients(loss, <list of variables>)

# grads_and_vars is a list of tuples (gradient, variable).  Do whatever you
# need to the 'gradient' part, for example cap them, etc.
capped_grads_and_vars = [(MyCapper(gv[0]), gv[1]) for gv in grads_and_vars]

# Ask the optimizer to apply the capped gradients.
opt.apply_gradients(capped_grads_and_vars)
\end{python}
minimize（）he compute\_gradients()接受一个gate\_gradients参数控制fradient应用中的并行度。

GATE\_NONE:并行的计算，应用梯度，在执行过正中最大化并行程度，在结果中一些非重复性的代价。例如两个梯度的矩阵乘法依赖于输入值:GATE\_NONE可能被应用到输入前其他梯度被计算导致非重复性的结果。

GATE\_OP:对于每个Op，在他们使用之前确保所有的梯度被计算了。为了避免Op的race 为多个输入生成梯度condition，这里梯度依赖于输入。

GATE\_GRAPH:确保在它们任何一个被使用前所有变量的梯度被计算，提供了最小的并行化但是如果你想在应用他们之前处理所有的梯度这是很有用的。
\textbf{Slots}\newline
像MomentumOptimizer和AdagradOptimizer之类的优化器子类，结合变量训练分配管理额外的变量。这称为Slots，Slots有名字，你可以要求优化器它使用的名字。当你有一个slot名字你可以对变量要求优化器创建保留slot值。当你调试训练算法报告slots统计信息时很管用。
\textbf{方法}\newline
\textbf{\_\_init\_\_}
\begin{python}
__init__(
    use_locking,
    name
)
\end{python}
创建一个新的优化器，他必须通过子类的构造体调用。\newline
参数:
\begin{itemize}
	\item use\_locking:bool,如果为True用lock阻止当前变量更新。
	\item name:非空字符串为optimizer创建的累加器的名字。
	\item ValueError:名字格式不对。
\end{itemize}
\textbf{apply\_gradients}
\begin{python}
apply_gradients(
    grads_and_vars,
    global_step=None,
    name=None
)
\end{python}
应用梯度到变量尚，这是minimize()的第二部分，他返回应用梯度的Op。

参数:
\begin{itemize}
	\item grads\_add\_vars:返回compute\_gradients()的(梯度,变量)对列表。
	\item global\_step:变量被更新后此选项变量增加1.
	\item name:返回操作的名字。默认传递名字给优化器构造函数。
	\item [S] 返回:应用指定梯度的操作。如果global\_step不是None，操作增加global\_step
	\item 异常
	\begin{itemize}
		\item TypeError:如果grads\_and\_vars不对。
		\item ValueError:如果变量的梯度为none。
	\end{itemize}
\end{itemize}
\textbf{computer\_gradients}
\begin{python}
compute_gradients(
    loss,
    var_list=None,
    gate_gradients=GATE_OP,
    aggregation_method=None,
    colocate_gradients_with_ops=False,
    grad_loss=None
)
\end{python}
计算损失关于bar\_list的梯度，第一部分是minimize().返回一个(gradient,variable)对这里gradient是变量的梯度。注意梯度可以是tensor，IndexedSlices或者None（如果没有变量的梯度）
\begin{itemize}
	\item loss:包含需要最小化的值的tensor。
	\item var\_list:tf.Variable更新最小化loss的列表或者元组。默认图中的变量列表在GraphKey.TRAINABLE\_VARIABLES下。
	\item gate\_gradients:如何gate梯度计算，可以是GATE\_NONE,GATE\_OP或者是GATE\_GRAPH。
	\item aggregation\_method:指定结合梯度的方法，可用值定义在AggregationMethod。
	\item colocate\_gradients\_with\_ops:如果为True，尝试随着相关操作colocating梯度。
	\item grad\_loss:一个保持住loss梯度的Tensor。
	\item[S] 返回(gradient,variable)对，变量总是被呈现但是梯度可能是None。
\end{itemize}
\subsection{tf.train.slice\_input\_producer}
\begin{lstlisting}[language=Python]
slice_input_producer(
    tensor_list,
    num_epochs=None,
    shuffle=True,
    seed=None,
    capacity=32,
    shared_name=None,
    name=None
)
\end{lstlisting}
生成一个在tensor\_list的Tensor切片。用队列实现，一个QueueRunner被添加到当前的Graph‘s的QUEUE\_RUNNER集合。
参数:
\begin{itemize}
\item tensor\_list:一个Tensor对象的列表，每个tensor\_list中的Tensor在第一个维度上必须有相同的大小。
\item num\_epochs:一个整数(可选)如果指定，slice\_input\_producer每num\_epochs产生一个切片，直到出现OutOfRange错误。如果不指定，slice\_input\_producer可以无限次数的循环切片
\item shuffle:Boolean。如果为true，整数在每个epoch被随机打乱
\item seed:一个整数(可选)，如果shuffle == True用一个种子
\item Capacity:一个整数，设置队列的容量
\item shared\_name:(可选),如果设置，队列将被在多个会话中通过名字共享
\item name:可选，操作的名字
\item[Returns]:一个tensor列表，元素是tensor\_list中的元素。如果在tensor\_list中的tensor形状为[N,a,b,...,z]，对应的输出维度的形状为[a,b,...,z]
\item[ValueError]:如果slice\_input\_producer从tensor\_list什么都没有产生
\end{itemize}
\subsection{tf.train.shuffle\_batch}
\begin{lstlisting}[language=Python]
shuffle_batch(
    tensors,
    batch_size,
    capacity,
    min_after_dequeue,
    num_threads=1,
    seed=None,
    enqueue_many=False,
    shapes=None,
    allow_smaller_final_batch=False,
    shared_name=None,
    name=None
)
\end{lstlisting}
通过随机大暖tensor创建batches。这个函数添加如下到Graph:
\begin{itemize}
\item 从tensors出兑打乱队列
\item dequeue\_many从queue创建batches
\item 从tensor出队一个QueueRunner到QUEUE\_RUNNER集合
\end{itemize}
如果enqueue\_many是False，tensors假设代表一个单个样本。一个输入tensor形状[x,y,z]将成为[batch\_size,x,y,z]的输出

如果enqueue\_manny是True，tensors假设表示一批样本，这里的第一位是样本的索引。tensors的所有成员应该在第一维度上有相同的代销。如果输入tensor有形状[*,x,y,z],输出将有形状[batch\_size,x,y,z]

capacity:参数控制允许队列有多长
返回操作是一个出队操作，如果输入队列耗尽将抛出tf.errors.OutOfRangeError。如果这操作被田东另一个输入队列，他的队列将捕获异常，然而，如果这个操作被用在你的主线程中你将响应，自己捕获。

例如:
\begin{lstlisting}[language=Python]
# Creates batches of 32 images and 32 labels.
image_batch, label_batch = tf.train.shuffle_batch(
      [single_image, single_label],
      batch_size=32,
      num_threads=4,
      capacity=50000,
      min_after_dequeue=10000)
\end{lstlisting}
你必须确保(1)shapes参数被传递，(2)所有在tensors的tensor必须有完整定义的形状。ValueError将在两者之中有一个不满足时出现。
如果allow\_maller\_final\_batch是True,没有足够的元素填充进batch时一个比batch\_size小的值被返回，否则未处理的元素江北丢弃/另外，所有输出tensor的静态shape，正如通过shae参数访问将有一个值为None的Dimension，依赖于fixed\_batch\_size的操作将失败。

参数:
\begin{lstlisting}[language=Python]
\item tensors:一个入队的列表或者字典
\item batch\_size:一个从queue获取的批大小
\item capacity:一个整数，队列的最大元素
\item min\_after\_dequeue:出队后在队列中最数目，用于确保一个混合元素的等级
\item num\_threads:一个些线程确保入伏你tensor\_list
\item seed:一个在queue中的随机种子
\item enqueue\_many:是否在tensor\_list中的每个tensor数一个样本
\item shape:(可选)每个样本的形状。默认从tensor\_list推断
\item allow\_smaller\_final\_batch:(可选)Boolean，如果为True，如果队列中没有足够的元素允许最后的batch变小
\item shared\_name:(可选)如果设置，队列将在给定名字下在多个会话上共享。
\item name:（可选）操作的名字
\item[Returns]:一个tensors类型的列表或tensors字典
\item[Raises]:ValueError:如果shape没有指定，而且不能通过tensors的元素推断
\end{lstlisting}

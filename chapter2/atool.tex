\section{TensorFlow用其他语言}
\subsection{背景}
这个文档是针对那些想要在其他语言中创造或者开发TensorFlow功能的开发者。它描述了TensorFlow的特性和在其它编程语言中使用的推荐步骤。

Python是TensorFlow支持的首选语言，当前支持的特性最多。更多的函数正在被移植进TensorFlow（C++实现）核心通过\href{https://www.github.com/tensorflow/tensorflow/blob/r1.4/tensorflow/c/c_api.h}{C API}，客户端语言应该用语言的\href{https://en.wikipedia.org/wiki/Foreign_function_interface}{外部函数接口}调用这个\href{https://www.github.com/tensorflow/tensorflow/blob/r1.4/tensorflow/c/c_api.h}{C API}提供TensorFlow函数。
\subsection{概览}
在其它编程语言中提供TensorFlow功能可能分为下面多种情况:
\begin{itemize}
\item 运行一个预先定义好的图:给一个GraphDef(或者MetaGraphDef)protocol消息，能创建一个绘画，运行查询得到tensor结果。对于想上手机app或者服务器来说通过预先训练好的模型推理足够的
\item 图接口:每定义一个TensorFlow操作至少定义一个函数或者添加一个操作到图上。理想的这些函数将被自动生成因此他们保持同步直到操作的定义被修改
\item 梯度(AKA自动微分)：给一个图和输入输出操作列表，添加操作到图上计算输出与对应输入的偏微分(梯度)。对于图上的类似操作允许定义梯度函数
\item 函数:定义一个在GraphDef中能被多处调用的子图。在FunctionDefLibrary定义一个FunctionDef包含在GraphDef
\item 控制流:如果用户指定子图，构造if和while。最好的情况是他们和梯度一起工作
\item 神经网络库:一些组件结合在一起支持创建神经网络模型和训练他们(可能是分布式设置)。在其它语言有这些将很方便，当前没有计划支持Python外的其他语言。这些库同上面这些特性的包装器
\end{itemize}
在一个小的，语言绑定应该支持运行一个预先定义的图，但是多数应该支持图的构建。TensorFlow Python API提供所有的这些特性。
\subsection{当前状态}
新的语言支持应该建立的\href{https://www.github.com/tensorflow/tensorflow/blob/r1.4/tensorflow/c/c_api.h}{C API}之上。然而，正如下表所见，不是所有的函数在C中都可用。为\href{https://www.github.com/tensorflow/tensorflow/blob/r1.4/tensorflow/c/c_api.h}{C API}提供更多支持的计划正在进行
\begin{table}[!h]
	\begin{tabular}{|p{4cm}|p{6cm}|p{4.7cm}|}
\hline
特性&Python&C\\
\hline
运行一个预定义的图&tf.import\_graph\_def, tf.Session&	TF\_GraphImportGraphDef, TF\_NewSession\\
\hline
结合生成的操作构造图&Yes	&Yes (The C API supports client languages that do this)\\
\hline
梯度&tf.gradients	&\\
\hline
函数&tf.python.framework.function.Defun&\\
\hline
控制流&tf.cond, tf.while\_loop&\\	
\hline
神经网络库&tf.train, tf.nn, tf.contrib.layers, tf.contrib.slim&\\
\hline
\end{tabular}
	\caption{语言支持}
	\label{tab:ex7_1}
\end{table}
\newline
\textbf{推荐方法}\newline
\subsection{运行一个预定义的图}
语言绑定希望定义下面的类:
\begin{itemize}
\item Graph：一个图表示一个TensorFlow计算。有一些对应C API的TF\_Graph的操作组成，主要用作创建操作和启动会话的参数。在图(TF\_GraphNextOperation)上也支持迭代,通过名字(TF\_GraphOperationByName)查看操作转换为或者从GraphDef rotocol消息(TF\_GraphToGraphDef和TF\_GraphImportGraphDef)转换
\item Operation：表示图上的一个计算节点。对应C API中的TF\_Operation
\item Output:表示图中操作的输出。有一个DataType(和最终的形状)也许作为输入参数传送给一个函数进行加操作，或者对于一个Session的Run（）方法获取tensor作为输出。对应C API的TF\_Output。
\item Session:表示TensorFlow运行环境的类似实例的一个客户端呢。主要工作是构造一个Graph和在图上调用Run（）选项。对应C API中的一个TF\_Session。
\item Tensor表示数据类型为DataType的N维数组。获取数据进出Session的Run()调用。对应C API的TF\_Tensor。
\item DataType:一些TensorFlow支持的可能的类型，对应C API中的TF\_DataType和Python API中的dtype对应
\end{itemize}
\subsection{图的构造}
TensorFlow有一些操作和列表不是静态的，因此我们推荐通过手写生成添加操作到图上(军官厚些一些是一个好的找出生成器应该生成什么)，需要生成一个包含一个OpDef protocol消息的函数。有一些方法获取注册操作的OpDefs列表:
\begin{itemize}
\item 在CAPI中的TF\_GetAllOpList获取所有祖册的OpDef protocol消息。这可能用用于客户端语言写生成器。这要求客户端语言为了解释OpDef消息有Protocol buffer支持
\item  C++函数OpRegistry::Global()->GetRegisteredOps()返回所有注册的OpDef的相同的列表（定义在tensorflow/core/framework/op.h）。这可以用与在C++（对于没有protocol buffer 支持的语言来说特别管用）
写生成器
\item 通过一个自动的处理ASCII序列版本被周期性的检查tensorflow/core/ops/ops.pbtxt
\begin{itemize}
\item 在CameICase中操作的名字。为了按照语言习惯生成函数。例如，如果语言使用snake\_case,使用CameICase态体操作的函数名称
\item 一个输入和输出列表。对于一字儿通过属性访问的多台，如在\href{https://www.tensorflow.org/extend/adding_an_op}{Adding an op}描述
\item 一些属性值是否认的。注意一些将被自动推断(如果他们由输入决定)，一些将可选（如果他们有一个默认），一些将要要求(非默认)，
\item 常用操作，输入，输出和非推理的属性的文档
\item 一些通过运行环境使用的领域可能被代码生成器胡萝卜
\end{itemize}
\end{itemize}
一个OpDef能转化为text函数添加用TF\_OperationDescription C API添加操作到图上(打包语言的外部函数接口)
\begin{itemize}
\item 开始TF\_NewOperation()创建TF\_OperationDescription*
\item 当输入（依赖于是否输入有一个列表类型）调用TF\_AddInput()或者TF\_AddInputList()
\item Call TF\_SetAttr*()函数设置非推理的属性。如果你不想覆盖默认值也许跳过属性
\item 如果需要设置选项范围
\begin{itemize}
\item TF\_SetDevice():抢播操作到指定设备
\item TF\_AddControlInput():添加请求在一个操作开始前另一个操作完成前
\item TF\_SetAttrString("\_kernel")设置内核标记（很少使用）
\item TF\_ColocateWith()布置一个操作
\end{itemize}
\item 调用TF\_FinishOperation()调用时，添加这个操作到图上，之后不能被修改
\end{itemize}
存在样本运行代码代码生成器作为构建程序(Bazel genrule)的一部分。带把生成能用一个自动的cron程序运行，可能在结果中检测。这在生成代码和OpDef的生成之间创建一个分歧进入仓库。但是对于go语言的go get和Rust cargoops希望代码之前生成期望被生成的情况。在最后，对于一些语言代码可能从tensorflow/core/ops/ops.pbtxt动态生成。
\subsection{处理常数}
如果用户可以提佛那个仓鼠给输入参数调用将边的更简单。生成的代码转化常熟为操作添加到图上用于作为输入对操作被实例化。
\subsection{可选参数}
如果语言允许对一个函数(想关键字参数（默认使用Python）)的可选参数，为可选属性，操作名称，设备，控制输入等使用阿门。在一些语言中这些选项参数可以用于动态scopes（像Python的with块）。没有这些特性，库也许如在C++版本的而TensorFlow API存储构建的样本。
\subsection{Name scopes}
用一些scope问津结构支持命名图的操作是一个好的想法，特别是考虑TensorBoard依赖它用合理的方式显示大图。Python和C++有不同的方法：在Python中，使用with块，目录作为名字的一部分。有一个本地线程栈结合scopes定义名字层级结构。名称的最新组件通过用户明确使用或者按照操作被默认添加。在C++中名字的目录部分存储在Scopes对象。NewSubScope()方法添加名字的一部分返回一个新的Scope。最新的名字的组件用WithOpName()方法设置，想Python默认的通过添加的操作命名。Scope对象明确的传递指定的名字。
\subsection{包装器}
确保一些函数的私有属性以至于包装器函数做一点额外的工作可能就被替代。这也给一个escape 从生成代码的scope外支持特性。

包装器的一个用法是支持SparseTensor输入输出，一个SparseTensor是一个三个dense tensor的SparseTensor：索引，值，和形状。值是响亮的大小n，形状是响亮的rank，索引时一个矩阵[n,rank]。

另一个原因四用包装器保持状态。有一些这样的操作（例如变量）有一些同伴操作用于操作状态。在Python API有类用于操作，操作的构造体创建op，类的方法添加组件到图上操作状态。
\subsection{其它的考虑}
\begin{itemize}
		\item 它有些关键字用于和语言关键字（或者其他符号哦将残生困难，想库函数的名字或者生成的代码中的变量引用）冲突的关键字重命名操作函数和参数
		\item 通常对于添加一个Const操作到图上的函数是一个包装器因此生成的函数将有同于的DataType输入
\end{itemize}
\subsection{梯度，函数和控制流}
在这里支持梯度，函数和控制流操作在Python外的语言中不可用，当\href{https://www.github.com/tensorflow/tensorflow/blob/r1.4/tensorflow/c/c_api.h}{C API}提供需要的支持后这将被跟新。
\section{一个TensorFlow模型文见得开发者工具}
大多数用户不需要考虑TensorFlow存储在磁盘中文件的内部细节，但是如果你是一个工具开发者也许需要考虑。
例如你也许想分析模型或者在TensorFlow和其他格式之间转化。这个向导尝试向你解释一些你如何结合主要文件保存模型数据
的详细工作，确保容易开发一些工具。
\subsection{Protocol Buffers}
所有的TensorFlow的文件格式都是基于\href{https://developers.google.com/protocol-buffers/?hl=en}{Protocol Buffers}
,因此熟悉它是如何工作的很有价值。总结来说你在文本文件中定义数据结构，protobuf用C，Python或者你可以载入的其他语言生成
类，以一种友好的方式访问数据。我们经常认为Protocol Buffers作为protobufs，我们将在这个向导中使用用这个
这个惯例。
\subsection{GraphDef}
在TensorFlow中图对象是计算的基础。这包含一些节点组成的网络，每个节点代表一个操作，节点作为输入或者输出
被连接到其它节点，你可以通过调用as\_default\_def()保存它，返回一个GraphDef对象。

GraphDef类是一个定义在
\href{https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/framework/graph.proto}{tensorflow/core/framework/graph.proto}定义的ProtoBuf库创建的GraphDef类。
protobuf工具解析这个文本文件，生成代码载入，存储和操作图定义。如果你看到一个标准的TensorFlow
文件表示一个模型，它很可能包含protobuf代码保存的一些列序列化的的GraphDef对象。这个生成代码
用于保存和从文件中载入GraphDef文件。代码通常像下面这样载入这个模型
\lstinline[language=Python]{graph_def = graph_pb2.GraphDef()}
这行创建一个空的GraphDef对象，这个类从定义在grapg.proto中定义的本质文件创建。这个对象将从我们的文件
操作这个对象
\lstinline[language=Python]{with open(FLAGS.graph,"rb") as f}
这里我们传递进脚本一个路径获取文件
\begin{lstlisting}[language=Python]
if FLAGS.input_binary:
  graph_def.ParseFromString(f.read())
else:
  text_format.Merge(f.read(), graph_def)
\end{lstlisting}
\subsubsection{文本或者二进制}
事实上一个Protobuf我们可以存储进入两种不同的格式。文本格式是一个人类可读的形式，对于调试和编辑
十分方面，但是当有一些想权重的数值数据存储文本格式的文件将很大。你可以查看\href{https://github.com/tensorflow/tensorboard/blob/master/tensorboard/demo/data/graph_run_run2.pbtxt}{graph\_run\_run2.pbtxt}一个晓得样本
二进制文件相比文本文件小得多，尽管他们对我们不可读，在这个脚本，我们要求用户应用一个flag指示
师傅输入是二进制还是文本，因此我们知道正确的函数调用。你可以在\href{https://storage.googleapis.com/download.tensorflow.org/models/inception_v3_2016_08_28_frozen.pb.tar.gz}{inception\_v3 archive}
找到一个大型二进制文件样本。正如inception\_v3\_2016\_08\_28\_frozen.pb。
这个API本身可以有一些让人困惑，二进制调用事实上是ParseFromString(),然而我们用一个来自text\_format模块的使用函数
载入原始的文件。
\subsection{Nodes}
当你载入一个文件进入graph\_def变量时，你可以访问里面的数据，对于多数常见目的，重要的章节是节点列表
存储在节点数目中。这里的代码循换处理这个操作
\lstinline[language=Python]{for node in graph_def.node}
每个节点是一个NodeDef对象，定义在\href{https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/framework/node_def.proto}{tensorflow/core/framework/node\_def.proto}
,这是TensorFlow图的基本构建模块，每个定义的单独操作用输入连接，这里的number是NodeDef。
\subsubsection{name}
每个节点应该有一个独一无二的id，不被其他的图中的节点使用。如果你不指定一个作为你用Python API构建一个graph。
一个翻译到操作的名字上，想"MatMul"和单调递增的数连接，像为你选择"5"，名字被用于定义两个节点的连接。
当运行时为你的整个图设置输入和输出。
\subsubsection{op}
这定义了运行的操作，例如"Add","MatMul"或者"Conv2"。当一个图运行的时候，op的名字被在实现中注册查找。
注册通过调用REGISTER\_OP()宏操作。像在
\href{https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/ops/nn\_ops.cc}{tensorflow/core/ops/nn\_ops.cc}
\subsubsection{input}
一个字符串列表，列表中每个元素是其他节点的名字，通常跟着引号和输出端口。例如，一个节点和两个输入有一个像
["some\_node\_name","another\_node\_name"],等价于["some\_node\_name:0","another\_node\_name:0"]
定义节点的第一个输入作为节点的来自“some\_node\_name”的第一个输出，第二个输入来自"anather\_node\_name"的
第一个输出。
\subsubsection{device}
在大多数情况下你可以忽略这个，因为它定义在分布式环境下节点运行的位置，或者什么时候你想强制操作运行在
CPU或者GPU上。
\subsubsection{attr}
这是一个key/value存储节点的的属性。有节不变的参数，一些在像卷积滤波器的尺寸不改变。或者常数操作
的值。因为有如此多不同类型的属性值，对于字符串，对证书或者tensor值的数组，这里有一个分割的protobuf
文件定义的数据结构保存他们，在
\href{https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/framework/attr_value.proto}{tensorflow/core/framework/attr\_value.proto.}
每个属性有一个独一无二的名字字符串，期待的属性被列出然后操作被定义。如果一个属性被节点呈现。
但是如果一个操作定义中的默认的列表，默认被用于图的创建。你可以通过调用node.name访问所有的数据,node.op等
在Python中节点列表列表存储在GraphDef是一个模型架构的完整定义。
\subsection{Freezing}
一个让人难以理解的是训练中的权重不被存储在文件中。相反，他们被保存在
单个的checkpoint文件中，在graph中的Variable操作，当他们被初始化的时候
载入最新的值，因此有\href{https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/tools/freeze_graph.py}{freeze\_graph.py }
脚本接受一个图定义和一些checkpoint文件同时freeze他们到一个单独的文件，载入GraphDef的时候，
从最新的chepoint文件获取所有变量的值，当取代Variable操作和Const权重的数值数据存储在他的属性中，
然后剔除额外的不能用于钱箱推理的节点，vaocun输出结果的GraphDef进一个输出文件。
\subsection{权重格式}
如果你正在处理TensorFlow模型表达的神经网络模型，一个常见的问题是提取和解释权重值。一个通用的
方法存储他们，例如在图中freezed\_graph脚本创建，它作为Const操作包含作为Tensors的权重。这些定义在
\href{https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/framework/tensor.proto}{tensorflow/core/framework/tensor.proto}
包含数据的类型和尺寸，正如值本身。在Python中，你通过NodeDef从一个NodeDef操作获取TensorProto对象表达一个Const操作
通过调用想some\_node\_def.attr['value'].tensor。

这样给你一个权重数据的对象表达。数据将被存储在有suffix\_val的列表中作为对象类型的索引，例如float\_val对于32位浮点数据类型。
卷及权重的顺序经常对于处理在不同的框架中转化有技巧的。在TensorFlow中，滤波器权重对于Conv2D操作被存储在第二行。
期待的数据顺序[filter\_height,filter\_width,input\_depth,output\_depth],这里filter\_count随着
内存中一个邻接的值滑动平均增加。

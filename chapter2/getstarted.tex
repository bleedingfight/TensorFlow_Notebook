\chapter{Tensorflow进阶}
\section{模型存储和加载}
\begin{itemize}
\item 生成checkpoint文件，扩展名一般为.ckpt,通过在tf.train.Saver对象上调用Saver.saver()生成。它包含权重和其它程序中定义的变量，不包含
图的结构。如果需要在另一个程序中使用，需要重建图形结构，并告诉Tensorflow如何处理这些权重。
\item 生成(graph proto file)，这是一个二进制文件，扩展名一般是.pb,用tf.train.write\_graph()保存，只包含图形结构，不包含权重，然后使用tf.import\_graph\_def()加载
图形。
\end{itemize}
\section{tf.estimator快速导航}
TensorFlow的高级机器学习API(tf.estimator)使得配置，训练评价多种机器学习模型变得很简单，在这个导航中，你将用tf.estimator构造一个神经网络分类器在\href{https://en.wikipedia.org/wiki/Iris_flower_data_set}{iris data}基于花萼和花瓣的几何特性训练预测花的种类，你的代码按照如下5步执行:
\begin{enumerate}
    \item 载入CSV文件的训练测试数据到TensorFlowDataset
    \item 构造\href{https://www.tensorflow.org/api_docs/python/tf/estimator/DNNClassifier}{神经网络分类器}
    \item 用训练数据训练模型。
    \item 评估模型的精度。
    \item 分类新的样本
\end{enumerate}
\subsection{完成神经网络源代码}
\lstinputlisting[language=Python]{./chapter2/code/iris_dnn_demo.py}
下面的章节将详细介绍代码。
\subsection{载入CSV数据进入TensorFlow}
\href{https://en.wikipedia.org/wiki/Iris_flower_data_set}{Iris data set}包含有150行iris样本:Iris setosa, Iris virginica和Iris versicolor。
\begin{figure}[H]
\includegraphics[scale=0.2]{iris_three_species}
\end{figure}
每行的数据包括花萼的长宽，花瓣的长宽，花用整数代表0表示Iris setosa,1表示 Iris versicolor,2表示Iris virginica。
iris数据集已经被分成两部分
\begin{itemize}
    \item 120个样本的训练集\href{http://download.tensorflow.org/data/iris_training.csv}{iris\_training.csv}
    \item 30个样本的测试集\href{http://download.tensorflow.org/data/iris_test.csv}{iris\_test.csv}
\end{itemize}
导入需要的模型
\begin{lstlisting}[language=Python]
import os
import urllib.request

import numpy as np
import tensorflow as tf
#Ignore warning
os.environ['TF_CPP_MIN_LOG_LEVEL']='2' 

# Data sets
IRIS_TRAINING = "iris_training.csv"
IRIS_TRAINING_URL = "http://download.tensorflow.org/data/iris_training.csv"

IRIS_TEST = "iris_test.csv"
IRIS_TEST_URL = "http://download.tensorflow.org/data/iris_test.csv"
\end{lstlisting}
如果训练集和测试集没有被存储在本地，下载它们:
\begin{lstlisting}[language=Python]
if not os.path.exists(IRIS_TRAINING):
    raw = urllib.request.urlopen(IRIS_TRAINING_URL).read().decode('utf-8')
    with open(IRIS_TRAINING, "wb") as f:
      f.write(raw)

  if not os.path.exists(IRIS_TEST):
    raw = urllib.request.urlopen(IRIS_TEST_URL).read().decode('utf-8')
    with open(IRIS_TEST, "wb") as f:
      f.write(raw)
\end{lstlisting}
下一步用learn.dataset.base中的load\_csv\_with\_header()方法载入训练数据进入Dataset，load\_csv\_with\_header()方法接受三个参数:
\begin{itemize}
    \item filename:CSV文件的完成的路径加上文件名。
    \item target\_dtype:接收numpy datatype的数据集的目标值。
    \item feature\_dtype:接收numpy datatype类型的数据集的特征值。
\end{itemize}
这里的目标(你的训练模型的预测)是花的种类，值范围为0~2,因此合适的numpy数据类型是np.int。
\begin{lstlisting}[language=Python]
# Load datasets.
training_set = tf.contrib.learn.datasets.base.load_csv_with_header(
    filename=IRIS_TRAINING,
    target_dtype=np.int,
    features_dtype=np.float32)
test_set = tf.contrib.learn.datasets.base.load_csv_with_header(
    filename=IRIS_TEST,
    target_dtype=np.int,
    features_dtype=np.float32)
\end{lstlisting}
在tf.contrib.learn中的Dataset是\href{https://docs.python.org/2/library/collections.html#collections.namedtuple}{named tuples};你可以通过data和target访问特征数据和目标值，这里training\_set.data和training\_set.target包含训练集的特征数据和目标数据，对应的test\_set.data和test\_set.target包含测试集特征和目标。
\subsection{构造神经网络分类器}
tf.estimator提供多种预定义方法，称为Estimator，你可以通过它在你的数据上运行训练，评估操作，你可以实例化tf.estimator.DNNClassfier：
\begin{lstlisting}[language=Python]
# Specify that all features have real-value data
feature_columns = [tf.feature_column.numeric_column("x", shape=[4])]

# Build 3 layer DNN with 10, 20, 10 units respectively.
classifier = tf.estimator.DNNClassifier(feature_columns=feature_columns,
                                        hidden_units=[10, 20, 10],
                                        n_classes=3,
                                        model_dir="./iris_model")
\end{lstlisting}
上面的代码中首先定义模型的特征列，指定在数据集中的特征的数据类型。所有的特征数据是连续的，因此tf.feature\_column.number\_column是构造特征列的合适的函数，数据集中有4个特征，因此我们指定shape为[4]保持所有的数据,然后用下面的参数创建DNNClassfier分类器模型:
\begin{itemize}
    \item feature\_columns=feature\_columns,特征集合的列。
    \item hidden\_units=[10,20,10],三个\href{http://stats.stackexchange.com/questions/181/how-to-choose-the-number-of-hidden-layers-and-nodes-in-a-feedforward-neural-netw}{hidden layer}包含有10,20,10个神经元。
    \item n\_classes=3,三个目标类，对应三个iris种类。
    \item model\_dir=./iris\_model:训练模型中保存checkpoint文件的路径
\end{itemize}
\subsection{描述训练的输入pipline}
tf.estimator API用输入函数创建TensorFlow操作为模型生成数据，你可以用\newline
tf.estimator.numpy\_input\_fn生成输入pipeline:
\begin{lstlisting}[language=Python]
# Define the training inputs
train_input_fn = tf.estimator.inputs.numpy_input_fn(
    x={"x": np.array(training_set.data)},
    y=np.array(training_set.target),
    num_epochs=None,
    shuffle=True)
\end{lstlisting}
\subsection{为iris训练集拟合DNNClassfier}
现在我们已经配置好的classfier模型，你可以用train方法通过训练数据训练模型。传递train\_input\_fn作为input\_fn,这里训练步数为2000:
\begin{lstlisting}[language=Python]
# Train model.
classifier.train(input_fn=train_input_fn, steps=2000)
\end{lstlisting}
状态模型被保存在classfier,意味着你可以反复训练，例如下面是合适的:
\begin{lstlisting}[language=Python]
classifier.train(input_fn=train_input_fn, steps=1000)
classifier.train(input_fn=train_input_fn, steps=1000)
\end{lstlisting}
然而，如果你在训练的时候跟踪模型，你可以用TensorFlow \href{https://www.tensorflow.org/api_docs/python/tf/train/SessionRunHook}{SessionRunHook}执行采集操作.
\subsection{评估模型的精度}
你可以在iris训练集上训练你的DNNClassfier模型；现在你可以在测试集上用evaluate检查它在测试集上个精确度。evaluate返回一个评估结果的字典，下面的代码传递irish测数据给test\_set.data和test\_set.target评估和从结果中打印。
\begin{lstlisting}[language=Python]
# Define the test inputs
test_input_fn = tf.estimator.inputs.numpy_input_fn(
    x={"x": np.array(test_set.data)},
    y=np.array(test_set.target),
    num_epochs=1,
    shuffle=False)

# Evaluate accuracy.
accuracy_score = classifier.evaluate(input_fn=test_input_fn)["accuracy"]

print("\nTest Accuracy: {0:f}\n".format(accuracy_score))
\end{lstlisting}
\begin{quote}
\emph{这里num\_epochs=1参数对于numpy\_input\_fn是很重要的。test\_input\_fn将在数据上迭代一次然后报出OutOfRangeError,这个错误通知分类器停止评估，因此它将计算输入一次}
\end{quote}
然后你可以运行完整的脚本，它将打印出:
\begin{lstlisting}[language=Python]
Test Accuracy: 0.966667
\end{lstlisting}
你的精度结果可能有点不同但是应该大于90\%。
\subsection{分类新的样本}
用estimator的predict()方法分类新的样本，例如你有两个新的花的样本:\par
\begin{table}
\centering
\begin{tabular}{|c|c|c|c|}
\hline
花萼长度&花萼宽度&花瓣长度&花瓣宽度\\
\hline
6.4&3.2&4.5&1.5\\
\hline
5.8&3.1&5.0&1.7\\
\hline
\end{tabular}
\end{table}
\par
你可以用predict(方法预测结果，predict返回一个词典生成器，生成器可以容易的被转化成列表，下面的代码访问和打印预测的分类:
\begin{lstlisting}[language=Python]
# Classify two new flower samples.
new_samples = np.array(
    [[6.4, 3.2, 4.5, 1.5],
     [5.8, 3.1, 5.0, 1.7]], dtype=np.float32)
predict_input_fn = tf.estimator.inputs.numpy_input_fn(
    x={"x": new_samples},
    num_epochs=1,
    shuffle=False)

predictions = list(classifier.predict(input_fn=predict_input_fn))
predicted_classes = [p["classes"] for p in predictions]

print(
    "New Samples, Class Predictions:    {}\n"
    .format(predicted_classes))
\end{lstlisting}
你应该得到如下结果
\begin{lstlisting}[language=Python]
New Samples, Class Predictions:    [1 2]
\end{lstlisting}
结果预测样本是 Iris versicolor, Iris virginica。
\section{用tf.estimator创建一个输入函数}
在这个导航中向你介绍在tf.estimator创建一个输入函数。你将看到如何构造一个input\_fn去处理和输入数据进你的模型，然后模型将为神经网络回归器实现一个input\_fn函数，评估预测房价数据
\subsection{用input\_fn自定义Pipeline}
input\_fn被用来传递特征和目标数据到Estimator的train,evaluate,predict方法。
\begin{lstlisting}[language=Python]
import numpy as np

training_set = tf.contrib.learn.datasets.base.load_csv_with_header(
    filename=IRIS_TRAINING, target_dtype=np.int, features_dtype=np.float32)

train_input_fn = tf.estimator.inputs.numpy_input_fn(
    x={"x": np.array(training_set.data)},
    y=np.array(training_set.target),
    num_epochs=None,
    shuffle=True)

classifier.train(input_fn=train_input_fn, steps=2000)
\end{lstlisting}
\subsection{input\_fn的分解}
下面的代码描述了输入函数的基本结构:
\begin{lstlisting}[language=Python]
def my_input_fn():

    # Preprocess your data here...

    # ...then return 1) a mapping of feature columns to Tensors with
    # the corresponding feature data, and 2) a Tensor containing labels
    return feature_cols, labels
\end{lstlisting}
输入函数的函数体包含指定处理你的输入数据的逻辑，像数据清洗和\href{https://en.wikipedia.org/wiki/Feature_scaling}{特征缩放}，输入函数必须返回两个包含最终的标签和特征的数据输入进你的模型:

featrue\_cols:一个包含有映射特征列名字为Tensor包含有特征数据的键值(key/value)对。

labels:一个包含有你的标签的值(你的模型想要预测的值)
\subsection{转换特征数据为Tensor}
如果你的feature/label数据是一个python数据，或者pandas dateframe或者numpy数组，你可以用下面的方法构造
input\_fn:
\begin{lstlisting}[language=Python]
import numpy as np
# numpy input_fn.
my_input_fn = tf.estimator.inputs.numpy_input_fn(
    x={"x": np.array(x_data)},
    y=np.array(y_data),
    ...)
\end{lstlisting}
\begin{lstlisting}[language=Python]
import pandas as pd
# pandas input_fn.
my_input_fn = tf.estimator.inputs.pandas_input_fn(
    x=pd.DataFrame({"x": x_data}),
    y=pd.Series(y_data),
    ...)
\end{lstlisting}
对于\href{https://en.wikipedia.org/wiki/Sparse_matrix}{稀疏,分类数据},你将需要填入下面三个参数:
\begin{itemize}
    \item dense\_shape:形状tensor。每个维度的列表的索引。例如dense\_shape=[3,6]指定二维tensor,形状为$3\times6$,dense\_shape=[2,3,4]指定3维tensor,形状为$2\times3\times4$tensor,dense\_shape=[9]指定包含9个元素的一维tensor。
    \item indices:在你的包含有非零值的tensor的元素的索引。接受列表，列表中的每个元素是包含非0元素的索引。（例如[0,0]代表两维Tensor的第0行第0列。indices=[[1,3],[2,4]]指定索引为[1,3],[2,4]的元素有非零值。）
    \item values:一维值得tensor，values中的i对应indices中的i和它指定的值。例如给定值indices=[[1,3],[2,4]],参数values=[18,3.6],指定元素索引[1,3]的位置为18,[2,4]的值为3.6。
\end{itemize}
下面的代码定义一个二维$3\times5$的SparseTensor，索引为[0,1]的位置的值为6，[2,4]位置的值为0.5，其它值为0。
\begin{lstlisting}[language=Python]
sparse_tensor = tf.SparseTensor(indices=[[0,1], [2,4]],
                                values=[6, 0.5],
                                dense_shape=[3, 5])
\end{lstlisting}
对应的tensor：
\begin{lstlisting}[language=Python]
[[0, 6, 0, 0, 0]
 [0, 0, 0, 0, 0]
 [0, 0, 0, 0, 0.5]]
\end{lstlisting}
\subsection{传递input\_fn数据到你的模型}
为了输入数据给你的模型训练，你简单的传递你创建的输入函数给你的train操作:
\begin{lstlisting}[language=Python]
classifier.train(input_fn=my_input_fn, steps=2000)
\end{lstlisting}
注意input\_fn参数必须接受一个函数对象（例如input\_fn=input\_fn）,这意味着如果你在训练调用的时候传递参数给你的input\_fn，不是函数调用的返回值，正如下面的代码一样，你将得到TypeError：
\begin{lstlisting}[language=Python]
classifier.train(input_fn=my_input_fn(training_set), steps=2000)
\end{lstlisting}
然而如果你想参数化你的输入函数，有其它的方法能做到，你可以实现一个包装器函数不接受参数input\_fn用它实现你想要的参数输入函数。
\begin{lstlisting}[language=Python]
def my_input_fn(data_set):
  ...

def my_input_fn_training_set():
  return my_input_fn(training_set)

classifier.train(input_fn=my_input_fn_training_set, steps=2000)
\end{lstlisting}
你同样可以用Python的\href{https://docs.python.org/2/library/functools.html#functools.partial}{function.pattial}函数构造一个新的参数固定的函数对象。
\begin{lstlisting}[language=Python]
classifier.train(
    input_fn=functools.partial(my_input_fn, data_set=training_set),
    steps=2000)
\end{lstlisting}
第三个选择是用lambda表达式包装你的input\_fn函数传递它给你的input\_fn参数:
\begin{lstlisting}[language=Python]
classifier.train(input_fn=lambda: my_input_fn(training_set), steps=2000)
\end{lstlisting}
用上面的方法的一个很大的好处是为你的数据集接受参数，你可以通过改变数据集参数传递相同的input\_fn函数给evaluate和prediction操作:
\begin{lstlisting}[language=Python]
classifier.evaluate(input_fn=lambda: my_input_fn(test_set), steps=2000)
\end{lstlisting}
这种方法加强的代码的维护性:不需要定义多的input\_fn函数(例如input\_fn\_train,\newline
input\_fn\_test,input\_fn\_prediction)给每个操作，最终你可以用tf.estimator.inputs中的方法从numpy或者pandas数据集创建input\_fn。另一个好处是你可以用更多的参数，像num\_epochs和shuffle控制input\_fn如何在数据上迭代，
\begin{lstlisting}[language=Python]
import pandas as pd

def get_input_fn_from_pandas(data_set, num_epochs=None, shuffle=True):
  return tf.estimator.inputs.pandas_input_fn(
      x=pd.DataFrame(...),
      y=pd.Series(...),
      num_epochs=num_epochs,
      shuffle=shuffle)
\end{lstlisting}
\begin{lstlisting}[language=Python]
import numpy as np

def get_input_fn_from_numpy(data_set, num_epochs=None, shuffle=True):
  return tf.estimator.inputs.numpy_input_fn(
      x={...},
      y=np.array(...),
      num_epochs=num_epochs,
      shuffle=shuffle)
\end{lstlisting}
\subsection{波士顿房价的神经网络模型}
接下来的导航，你将写输入函数处理从\href{https://archive.ics.uci.edu/ml/datasets/Housing}{UCI Gousing Data Set}获取的数据集的子集，传递数据给神经网络回归器预测房价
你将用于训练的神经网络包含下面的子集Boston CSV data sets包含下面\href{https://archive.ics.uci.edu/ml/machine-learning-databases/housing/housing.names}{特征数据}\par

\begin{tabular}{|c|c|}
\hline
特征&描述\\
\hline
CRIM&人均犯罪率\\
\hline
ZN&居住地面积划分为25000平方英尺一块\\
\hline
INDUS&非商业用地的一部分\\
\hline
NOX&一氧化氮的浓度为千万分之一\\
\hline
RM&每个房子的房间数\\
\hline
AGE&1940年前自有居民的比例\\
\hline
DIS&离波士顿就业中心的距离\\
\hline
TAX&每10000美元的税率\\
\hline
PTRATIO&学生老师的比率\\
\hline
\end{tabular}
\subsection{建立}
下载数据集\href{https://github.com/bleedingfight/TF_Code/blob/master/boston_price/boston_train.csv}{boston\_train.csv},\href{https://github.com/bleedingfight/TF_Code/blob/master/boston_price/boston_test.csv}{boston\_test.csv}和\href{https://github.com/bleedingfight/TF_Code/blob/master/boston_price/boston_predict.csv}{boston\_predict.csv}
\subsection{导入的房子数据}
\begin{lstlisting}[language=Python]
from __future__ import absolute_import
from __future__ import division
from __future__ import print_function

import itertools

import pandas as pd
import tensorflow as tf

tf.logging.set_verbosity(tf.logging.INFO)
\end{lstlisting}
给CULUMNS中的数据定义名字，区别于标签中的特征，定义FEATURES和LABEL，读入CSV文件到pandas DataFrame:
\begin{lstlisting}[language=Python]
COLUMNS = ["crim", "zn", "indus", "nox", "rm", "age",
           "dis", "tax", "ptratio", "medv"]
FEATURES = ["crim", "zn", "indus", "nox", "rm",
            "age", "dis", "tax", "ptratio"]
LABEL = "medv"

training_set = pd.read_csv("boston_train.csv", skipinitialspace=True,
                           skiprows=1, names=COLUMNS)
test_set = pd.read_csv("boston_test.csv", skipinitialspace=True,
                       skiprows=1, names=COLUMNS)
prediction_set = pd.read_csv("boston_predict.csv", skipinitialspace=True,
                             skiprows=1, names=COLUMNS)
\end{lstlisting}
\subsection{定义特征列创建回归器}
下一步是为输入数据创建FeatureColumn，数据的格式指定用于训练的特征集，因为所有在房价数据集中的的特征包含连续的值，你可以用tf.contrib.layers.real\_valued\_column()创建它们的FeatureColumn：
\begin{lstlisting}[language=Python]
feature_cols = [tf.feature_column.numeric_column(k) for k in FEATURES]
\end{lstlisting}
现在初始化一个神经网络回归模型的实体DNNRegressor，你需要提供两个参数:hidden\_units指定每个隐藏层的节点数(这里的两层，每层10个节点)和feature\_columns：包含FeatureColumns
\begin{lstlisting}[language=Python]
regressor = tf.estimator.DNNRegressor(feature_columns=feature_cols,
                                      hidden_units=[10, 10],
                                      model_dir="/boston_model")
\end{lstlisting}
\subsection{构建input\_fn}
传递输入数据给regressor,写一个factory方法接受pandas DataFrame返回一个input\_fn:
\begin{lstlisting}[language=Python]
def get_input_fn(data_set, num_epochs=None, shuffle=True):
  return tf.estimator.inputs.pandas_input_fn(
      x=pd.DataFrame({k: data_set[k].values for k in FEATURES}),
      y = pd.Series(data_set[LABEL].values),
      num_epochs=num_epochs,
      shuffle=shuffle)
\end{lstlisting}
注意输入数据被传递给input\_fn的data\_set参数，这意味着函数可以处理任何你导入的的DataFrame,training\_set,test\_set和prediction\_set。提供两个额外的参数num\_epochs(控制在数据上的迭代次数)训练的时候设置为None，因此input\_fn保持返回值知道训练步数到达，为了平局和测试设置为1，因此input\_fn将在数据上迭代然后抛出OutOfRangeError,错误将通知Estimator停止评估或者预测:shuffle（是否打乱数据）。对于评估和预测，设置为False，因此input\_fn在数据上顺序迭代，对于训练设置为True。
\subsection{训练回归器}
为了训练神经网络回归器，用training\_set传递给input\_fn运行train：
\begin{lstlisting}[language=Python]
regressor.train(input_fn=get_input_fn(training_set), steps=5000)
\end{lstlisting}
你应该能看到类似的输出，每100步报告训练的损失:
\begin{lstlisting}[language=Python]
INFO:tensorflow:Create CheckpointSaverHook.
INFO:tensorflow:Saving checkpoints for 1 into ./boston_model_demo/model.ckpt.
INFO:tensorflow:loss = 58842.2, step = 1
INFO:tensorflow:global_step/sec: 379.343
INFO:tensorflow:loss = 10089.6, step = 101 (0.264 sec)
INFO:tensorflow:global_step/sec: 414.38
INFO:tensorflow:loss = 12056.9, step = 201 (0.241 sec)
INFO:tensorflow:global_step/sec: 417.317
...
INFO:tensorflow:loss = 2884.69, step = 4801 (0.271 sec)
INFO:tensorflow:global_step/sec: 388.267
INFO:tensorflow:loss = 5111.75, step = 4901 (0.257 sec)
INFO:tensorflow:Saving checkpoints for 5000 into ./boston_model_demo/model.ckpt.
INFO:tensorflow:Loss for final step: 4082.04.
INFO:tensorflow:Starting evaluation at 2017-11-21-05:42:23
INFO:tensorflow:Restoring parameters from ./boston_model_demo/model.ckpt-5000
INFO:tensorflow:Finished evaluation at 2017-11-21-05:42:23
INFO:tensorflow:Saving dict for global step 5000: average_loss = 13.7577, global_step = 5000, loss = 1375.77
Loss: 1375.769165
INFO:tensorflow:Restoring parameters from ./boston_model_demo/model.ckpt-5000
\end{lstlisting}
\subsection{评估模型}
下一步看看模型在测试数据及上的性能，运行evaluate,传递test\_set到input\_fn：
\begin{lstlisting}[language=Python]
ev = regressor.evaluate(
    input_fn=get_input_fn(test_set, num_epochs=1, shuffle=False))
\end{lstlisting}
从ev结果返回损失的，打印:
\begin{lstlisting}[language=Python]
loss_score = ev["loss"]
print("Loss: {0:f}".format(loss_score))
\end{lstlisting}
你应该能看到下面的结果:
\begin{lstlisting}[language=Python]
Loss: 1375.769165
\end{lstlisting}
\subsection{做出预测}
最后你可以用模型在给定的预测包含特征数据没有标签的数据集上预测房价
\begin{lstlisting}[language=Python]
y = regressor.predict(
    input_fn=get_input_fn(prediction_set, num_epochs=1, shuffle=False))
# .predict() returns an iterator of dicts; convert to a list and print
# predictions
predictions = list(p["predictions"] for p in itertools.islice(y, 6))
print("Predictions: {}".format(str(predictions))
\end{lstlisting}
你应该得到包含6个房价值(单位是千美元)
\begin{lstlisting}[language=Python]
Predictions: [array([ 34.70497513], dtype=float32), array([ 17.63309288], dtype=float32), array([ 23.71421814], dtype=float32), array([ 35.60274506], dtype=float32), array([ 15.12172413], dtype=float32), array([ 19.79147911], dtype=float32)]
\end{lstlisting}
\section{tf.contrib.learn采集和监控基础}
当我们训练模型的时候实时跟踪评估是有价值的，在这个导航中，你将学习如何用TensorFlow的采集能力和Monitor API在分类iris花的过程中省查程序。这个导航的代码基于上一上。
\subsection{建立}
\begin{lstlisting}[language=Python]
from __future__ import absolute_import
from __future__ import division
from __future__ import print_function

import os

import numpy as np
import tensorflow as tf

# Data sets
IRIS_TRAINING = os.path.join(os.path.dirname(__file__), "iris_training.csv")
IRIS_TEST = os.path.join(os.path.dirname(__file__), "iris_test.csv")

def main(unused_argv):
    # Load datasets.
    training_set = tf.contrib.learn.datasets.base.load_csv_with_header(
        filename=IRIS_TRAINING, target_dtype=np.int, features_dtype=np.float32)
    test_set = tf.contrib.learn.datasets.base.load_csv_with_header(
        filename=IRIS_TEST, target_dtype=np.int, features_dtype=np.float32)

    # Specify that all features have real-value data
    feature_columns = [tf.contrib.layers.real_valued_column("", dimension=4)]

    # Build 3 layer DNN with 10, 20, 10 units respectively.
    classifier = tf.contrib.learn.DNNClassifier(feature_columns=feature_columns,
                                                hidden_units=[10, 20, 10],
                                                n_classes=3,
                                                model_dir="/tmp/iris_model")

    # Fit model.
    classifier.fit(x=training_set.data,
                   y=training_set.target,
                   steps=2000)

    # Evaluate accuracy.
    accuracy_score = classifier.evaluate(x=test_set.data,
                                         y=test_set.target)["accuracy"]
    print('Accuracy: {0:f}'.format(accuracy_score))

    # Classify two new flower samples.
    new_samples = np.array(
        [[6.4, 3.2, 4.5, 1.5], [5.8, 3.1, 5.0, 1.7]], dtype=float)
    y = list(classifier.predict(new_samples, as_iterable=True))
    print('Predictions: {}'.format(str(y)))

if __name__ == "__main__":
  tf.app.run()
\end{lstlisting}
复制上面的代码到一个文件下载相关的\href{http://download.tensorflow.org/data/iris_training.csv}{训练}和\href{http://download.tensorflow.org/data/iris_test.csv}{测试}数据在同一目录。下面你将更新代码增加采集和监控能力。
\subsection{概览}
上一张我们实现了一个神经网络分类器分类iris样本为三种类别，但是当代码运行的时候，输出没有跟踪训练进程，仅仅打印处结果:
\begin{lstlisting}[language=Python]
Accuracy: 0.933333
Predictions: [1 2]
\end{lstlisting}
没有任何采集,模型就像一个黑盒子，你不能看到TensorFlow随着梯度下降发生了什么，模型是否收敛或者是否审查决定是否应该\href{https://en.wikipedia.org/wiki/Early_stopping}{提前停止},处理这个问题的一个方法是分隔模型为多个fit调用在更小的时间步获得更精确的评估，然而日常使用不推荐因为它会极大地降低模型的训练。幸运的是tf.contrib.learn提供了另一个解:一个\href{https://www.tensorflow.org/api_docs/python/tf/contrib/learn/monitors}{Monitor API}设计用来帮助你在训练中采集度量和评估你的模型，下面的章节你将学习如何在TensorFlow中采集，建立ValidationMonitor做streaming评估，用TensorBoard可视化你的衡量标准。
\subsection{让你的TensorFlow能采集}
TensorFlow有5个不同的等级采集消息。分别是DEBUG，INFO，WARN，ERROR和FATAL，当你配置好级别后TensorFlow将输出所有和你级别和更高级别的相关消息。例如你设置级别为DEBUG你将从上面五个级别得到采集信息。默认，TensorFlow被被配置的采集级别为WARN,但是当跟踪模型训练时，你将想要调整级别为INFO，将提供额外的反馈作为fit操作，增加下面行到你的代码:
\begin{lstlisting}[language=Python]
tf.logging.set_verbosity(tf.logging.INFO)
\end{lstlisting}
当你运行代码的时候，你将看到如下额外的采集输出:
\begin{lstlisting}[language=Python]
INFO:tensorflow:loss = 1.18812, step = 1
INFO:tensorflow:loss = 0.210323, step = 101
INFO:tensorflow:loss = 0.109025, step = 201
\end{lstlisting}
在INFO级别采集tf.contrib.learn自动每100步输出\href{}{train-loss metric}到标准输出。
\subsection{配置Streaming评估的ValidationMonitor}
采集训练的损失对于帮你理解你的模型是否收敛是很有用的，但是如果你想了解训练过程发生了什么?tf.contrib.learn提供几个更高级别的Monitor，你可以添加到你的fit操作以进一步在模型训练中记录度量或者调试低级TensorFlow操作。包括:

\begin{table}[h!]
\centering
	\begin{tabular}{|p{3cm}|p{7cm}|}
\hline 
Monitor&描述\\
CaptureVariable&每n步保存一个指定变量的值到集合\\
PrintTensor&在每个训练步采集指定tensor的值。\\
SummarySave&用tf.summary.FileWriter在训练中每n步为一个指定的\href{https://www.tensorflow.org/api_docs/python/tf/Summary}{tf.Summary protocal buffer}\\
ValidationMonitor&在训练中每n步采集一个指定的评估方案，如果想要，在确定条件下实现early stopping。\\
\hline
\end{tabular}
\end{table}
\subsection{每N步评估}
对于神经网络分类器你也许想采集训练损失同时像评估测试数据,看看模型的泛化能力。你可以你结合配置一个ValidationMonitor和测试数据(test\_set.data,test\_set.target)，用every\_n\_steps设置评估的频率。every\_n\_steps默认值为100，这里设置every\_n\_step为50每50步评估模型训练。
\begin{lstlisting}[language=Python]
validation_monitor = tf.contrib.learn.monitors.ValidationMonitor(
    test_set.data,
    test_set.target,
    every_n_steps=50)
\end{lstlisting}
放这段代码在初始化classfier之前。ValidationMonitor依靠保存checkpoint文件指定计算操作，你将想要修改classifier去增加一个包含有save\_checkpointers\_secs的tf.contrib.learn.RunConfig(指定训练过程多少秒保存checkpoint)。因为iris数据集很小，这样训练和快，可以设置save\_checkpoints\_secs文件为1（每一秒保存一个ckeckpoint文件），确保一个高效的checkpoint。
\begin{lstlisting}[language=Python]
classifier = tf.contrib.learn.DNNClassifier(
    feature_columns=feature_columns,
    hidden_units=[10, 20, 10],
    n_classes=3,
    model_dir="/tmp/iris_model",
    config=tf.contrib.learn.RunConfig(save_checkpoints_secs=1))
\end{lstlisting}
注意model\_dir指定了一个可用的目录(/tmp/iris\_model)存储模型数据，这个路径相比自动生成的路径在之后将会很容易被访问，每次你运行代码的时候任何/tmp/iris\_model中的数据将被载入，模型将继续从上次停止的地方运行，为了重新训练模型在运行代码前先删掉/tmp/iris\_model,最后添加你的validation\_monitor,更新包含monitor参数的fit调用
\begin{lstlisting}[language=Python]
classifier.fit(x=training_set.data,
               y=training_set.target,
               steps=2000,
               monitors=[validation_monitor])
\end{lstlisting}
当你返回代码，你应该看到下面输出:
\begin{lstlisting}[language=Python]
INFO:tensorflow:Validation (step 50): loss = 1.71139, global_step = 0, accuracy = 0.266667
...
INFO:tensorflow:Validation (step 300): loss = 0.0714158, global_step = 268, accuracy = 0.966667
...
INFO:tensorflow:Validation (step 1750): loss = 0.0574449, global_step = 1729, accuracy = 0.966667
\end{lstlisting}
\subsection{用MetricSpec自定义评估方案}
默认没有评估方案指定，ValidationMonitor将采集损失和精度，但是你可以每50步自定义度量列表，为了指定明确的方案你在评估运行时指定明确的方案，你可以增加一个metrics参数到ValidationMonitor构造体，metrics结果一个字典(key/value)这里key是你想采集的度量的名字，相对应的值是MetricSpec对象，MetricSpec构造体接受4个参数:
\begin{itemize}
  \item metric\_fn:函数计算返回度量的值。这可以是tf.contrib.metric模型中预定义的可用的函数，像tf.contrib.streaming\_precision或者tf.contrib.metrics.streaming\_recall。你可以定义你的个性化的度量函数(必须接受predictions和labels作为参数，weights参数每选择性提供。函数必须返回下面两种格式的值)
  \begin{itemize}
    \item 一个tensor
    \item 一个操作对(value\_Op,update\_op)，这里value\_op返回metric值update\_op执行一个对应的操作更新内部模块的状态。
  \end{itemize}
  \item prediction\_key:包含模型返回的label的tensor，由模型的input\_fn指定，正如prediction\_key,如果input\_fn返回一个tensor或者单输入的字典，在导航中的iris例子，DNNClassfier没有input\_fn(x,y数据直接传递给fit),因此不需要提供label\_key。
  \item weights\_key:包含有metric\_fn权重输入的tensor。
\end{itemize}
下面的代码创建一个validation\_metric字典在模型评估中定义三个度量。
\begin{itemize}
  \item accuracy:用tf.contrib.metrics.streaming\_accuracy作为metric\_fn。
  \item prediction:用tf.contrib.metrics.streaming\_prediction作为metric\_fn
  \item recall:用tf.contrib.metrics.streaming\_recall作为metric\_fn
\end{itemize}
\begin{lstlisting}[language=Python]
validation_metrics = {
    "accuracy":
        tf.contrib.learn.MetricSpec(
            metric_fn=tf.contrib.metrics.streaming_accuracy,
            prediction_key=tf.contrib.learn.PredictionKey.CLASSES),
    "precision":
        tf.contrib.learn.MetricSpec(
            metric_fn=tf.contrib.metrics.streaming_precision,
            prediction_key=tf.contrib.learn.PredictionKey.CLASSES),
    "recall":
        tf.contrib.learn.MetricSpec(
            metric_fn=tf.contrib.metrics.streaming_recall,
            prediction_key=tf.contrib.learn.PredictionKey.CLASSES)
}
\end{lstlisting}
在ValidationMonitor构造体前增加上面的代码，然后ValidationMonitor构造体增加metrics参数去采集validation\_metrics中定义的 accuracy, precision和recall metrics(损失总是被采集不需要明确的指定)
\begin{lstlisting}[language=Python]
validation_monitor = tf.contrib.learn.monitors.ValidationMonitor(
    test_set.data,
    test_set.target,
    every_n_steps=50,
    metrics=validation_metrics)
\end{lstlisting}
返回代码你应该在你的输出日志中看到precision和recall:
\begin{lstlisting}[language=Python]
INFO:tensorflow:Validation (step 50): recall = 0.0, loss = 1.20626, global_step = 1, precision = 0.0, accuracy = 0.266667
...
INFO:tensorflow:Validation (step 600): recall = 1.0, loss = 0.0530696, global_step = 571, precision = 1.0, accuracy = 0.966667
...
INFO:tensorflow:Validation (step 1500): recall = 1.0, loss = 0.0617403, global_step = 1452, precision = 1.0, accuracy = 0.966667
\end{lstlisting}
\subsection{用ValidationMonitor提前终止}
注意上面的输出每600步模型已经得到precision和recall 1.0。抛出问题说明模型是否可以从early stopping中获得好处、另外采集运算的度量,当指定条件满足时ValidationMonitor容易通过下面的参数实现early stopping。

\begin{table}[h!]
\centering
\begin{tabular}{|p{6cm}|p{8cm}|}
		\hline
参数&描述\\
\hline
early\_stopping\_metrics&度量在给定的条件early\_stopping\_rounds和early\_stopping\_metric\_metric\_minimize下出发early stopping,默认是"loss"\\
\hline
early\_stopping\_metric\_minimize&如果想要模型行为最小化early\_stopping\_metric为True，如果想最大化它的值为False，默认为True\\
\hline
early\_stopping\_rounds&如果early\_stopping\_metric不见效或者增加，训练将被停止，默认为None表示永不提前停止\\
\hline
\end{tabular}
\end{table}
按照下面修改ValidationMonitor构造体，指定是否损失200步(early\_stopping\_rounds=200)没有减少,模型训练在到达这个点将立即终止，并不完成fit中指定的2000步。
\begin{lstlisting}[language=Python]
validation_monitor = tf.contrib.learn.monitors.ValidationMonitor(
    test_set.data,
    test_set.target,
    every_n_steps=50,
    metrics=validation_metrics,
    early_stopping_metric="loss",
    early_stopping_metric_minimize=True,
    early_stopping_rounds=200)
\end{lstlisting}
如果模型训练提前结束将返回下面的代码:
\begin{lstlisting}[language=Python]
...
INFO:tensorflow:Validation (step 1150): recall = 1.0, loss = 0.056436, global_step = 1119, precision = 1.0, accuracy = 0.966667
INFO:tensorflow:Stopping. Best step: 800 with loss = 0.048313818872.
\end{lstlisting}
特别是这里的训练步数为1150，对于之前200步，损失不在减少，总体800产生最小的损失与之对应测试数据集。这意味着额外的超参数标准被减少也许将进一步提升模型。
\subsection{用TensorBoard可视化采集数据}
读ValidationMonitor生成的日志提供了丰富的训练过程中的原始数据，可视化这些数据有助于得到更深的见解。例如精确度如何随着步数改变。你可以用TensorBoard设置，命令行参数logdir画图，运行下面的命令行tensorboard --logdir=/tmp/iris\_model/浏览器中导航到http://0.0.0.0:<6006>，如果你点击精确度范围，你将看到一个像下面的图，绘制精度和步数。
\begin{figure}[H]
\includegraphics[scale=0.1]{validation_monitor_tensorboard_accuracy.png}
\end{figure}

\section{TensorBoard Histogram Dashboard}
TensorBoard Histogram Dashboard 显示TensorFlow图中的Tensor如何随着时间变化。
\subsection{一个简单的例子}
正态分布变量，均值随着和时间移动。TensorFlow有一个操作tf.random\_normal可以完美的达到这个目的。正如通常情况下TensorBoard，我们将用summary op融合数据据。
在这种情况下'tf.summary.histogram'。
这里有一个代码段将生成一些包含正态分布直方图数据的总结，这里均值随着时间增大。
\begin{lstlisting}[language=Python]
import tensorflow as tf
k = tf.placeholder(tf.float32)
mean_moving_normal = tf.random_normal(shape=[1000], mean=(5*k), stddev=1)
summaries = tf.summary.histogram('normal/moving_mean',mean_moving_normal)
sess = tf.Session()
writer = tf.summary.FileWriter('./histogram_example')
N = 400
for step in range(N):
    k_val = step/float(N)
    summ = sess.run(summaries,feed_dict={k:k_val})
    writer.add_summary(summ,global_step=step)
\end{lstlisting}
在当前代码中运行下边的代码启动TensorFlow载入数据
\begin{lstlisting}[language=Python]
tensorboard --logdir=./histogram_example
\end{lstlisting}
\begin{center}
\begin{figure}[H]
\includegraphics[scale=0.5]{tb_hist1.png}
\end{figure}
\end{center}
tf.summary.histogram接受任一尺寸和大小的Tensor，压缩它们进入直方数据结构组成一些小的数据宽度和数量组层的bin将诶够，例如我们像组成数[0.5,1.1,1.3,2.2,2.9,2.99]成3个bin，我们可以创建三个bin：一个包含0到1之间的一切(0.5)，一个包含1-2(1.1,1.3)之间，一个包含2-3(2.2,2.9,2.99)

  TensorFlow用类是的方法创建bins，但是不想我们上面的例子，它不创建整数读额bins，瑞与大型数据，稀疏数据，这样的也许导致上千个bin，bins时指数分布时，一些bins相比于一些非常大数的bin接近于0。然而，可视化指数分布bin时一个技巧，如果高被编码为数量，bin宽度更大的空间，甚至它们有相同的元素，相比较之下统计数量使得豪赌比较变得可能，直方图采集数据仅均匀的bins，这可能导致不幸的人工操作。

在直方图可视化器的每一个切片显示为一个单个的直方图。切片安装步数组织。例老的切片(e.g. step 0)比较靠后变为更深，然而新的slices接近于前景色，颜色更轻，右边的y轴显示了步数。

你可以在直方图上滑动鼠标看到更多的详细星系。你如下面的图你可以看到直方图的时间不为177有一个bin中心在3.78有bin中有34.5个元素。
\begin{center}
\begin{figure}[H]
\includegraphics[scale=0.5]{tb_hist2.png}
\end{figure}
\end{center}
你也许注意到注意到直方切片在统计步数和时间上不总是偶数，这是因为TensorBoard用\href{https://en.wikipedia.org/wiki/Reservoir\_sampling}{reservoir sampling}保持直方图的子集，为了节约内存，Reservior sampling保证每个采样有一个相等的可能性被包含进去，但是因为它时一个随机算法，采样并不在每个偶数步发生。
\subsection{Overlay Mode}
控制面板上允许你打开直方图模式为offset为overlay。
在offset模式下，可视化转动45度，因此单个的直方图切片不再展开，而是所有的图华仔一个相同的y轴上。
\begin{center}
\begin{figure}[H]
\includegraphics[scale=0.5]{tb_hist3.png}
\end{figure}
\end{center}
现在表上的每个切片被线分开，y轴显示每个bucket项目数量，深色线时老的，早期的时间不，浅色线时最近的新的时间不，你可以用鼠标在表上查看更多的信息。
\begin{center}
\begin{figure}[H]
\includegraphics[scale=0.5]{tb_hist4.png}
\end{figure}
\end{center}

overlay可视化在你想直接比较不同直方图的数量。
\subsection{多个分布}
直方图控制面板对多分布下的可视化很有用，当我们通过链接两个不同的正态分布构造一个简单的二两分布，代码如下:
\begin{lstlisting}[language=Python]
import tensorflow as tf
k = tf.placeholder(tf.float32)
mean_moving_normal = tf.random_normal(shape=[1000],mean=(3*5),stddev=1)
tf.summary.histogram('normal/moving_mean',mean_moving_normal)
variance_shrinking_normal = tf.random_normal(shape=[100],mean=0,stddev=1-(k))
tf.summary.histogram('normal/shrinking_varance',variance_shrinking_normal)
normal_combined = tf.concat([mean_moving_normal,variance_shrinking_normal],0)
tf.summary.histogram('normal/bimodal',normal_combined)
summaris = tf.summary.merge_all()
sess = tf.Session()
writer = tf.summary.FileWriter('./histgram_example1')
N = 400
for step in range(N):
    k_val = step/float(N)
    summ = sess.run(summaris,feed_dict={k:k_val})
    writer.add_summary(summ,global_step=step)
\end{lstlisting}
上面的例子是滑动平均，现在我们已有一个收缩的变量分布。
\begin{center}
\begin{figure}[H]
\includegraphics[scale=0.6]{tb_hist5.png}
\end{figure}
\end{center}
当我们链接她们在一起，我们得到一个清晰解释分歧，二进制结构的表格:
\begin{center}
\begin{figure}[H]
\includegraphics[scale=0.5]{tb_hist6.png}
\end{figure}
\end{center}
\subsection{更多分布}
生成可视化更多分布，结合它们到表中:
\begin{lstlisting}[language=Python]
import tensorflow as tf
k = tf.placeholder(tf.float32)
# Make a normal distribution,with a shift mean
mean_moving_normal = tf.random_normal(shape=[1000],mean=(5*k),stddev=1)
tf.summary.histogram('normal/moving_mean',mean_moving_normal)
variance_shrinking_normal = tf.random_normal(shape=[1000],mean=0,stddev=1-(k))
tf.summary.histogram('normal/shinking_variance',variance_shrinking_normal)
normal_combined = tf.concat([mean_moving_normal,variance_shrinking_normal],0)
tf.summary.histogram("normal/bimodal",normal_combined)
#add gamma distribution
gamma = tf.random_gamma(shape=[1000],alpha=k)
tf.summary.histogram('gamma',gamma)
poisson = tf.random_poisson(shape=[1000],lam=k)
tf.summary.histogram('poisson',poisson)
#add a uniform distribution
uniform = tf.random_uniform(shape=[1000],maxval=k*10)
tf.summary.histogram('uniform',uniform)
#finnally combine everything together

all_distributions = [mean_moving_normal,variance_shrinking_normal,gamma,poisson,uniform]
all_combined = tf.concat(all_distributions,0)
tf.summary.histogram('all_combined',all_combined)
summaries = tf.summary.merge_all()
sess = tf.Session()
writer = tf.summary.FileWriter('./histogram_example2')
N = 400
for step in range(N):
    k_val = step/float(N)
    summ = sess.run(summaries,feed_dict={k:k_val})
    writer.add_summary(summ,global_step=step)
\end{lstlisting}
\begin{center}
\begin{figure}[H]
\includegraphics[scale=0.5]{tb_hist7.png}
\end{figure}
\end{center}
\begin{center}
\begin{figure}
\includegraphics[scale=0.5]{tb_hist9.png}
\end{figure}
\end{center}
\subsection{poisson分布}
\begin{center}
\begin{figure}[H]
\includegraphics[scale=0.5]{tb_hist10.png}
\end{figure}
\end{center}
poisson分布定义在整数上，因此所有被生成的值都是整数，直方图压缩移动数据到浮点bins，导致可视化在整数值上显示一点点突起。
\subsection{结合所有的数据到一张图向上}
\begin{center}
\begin{figure}[H]
\includegraphics[scale=0.5]{tb_hist11.png}
\end{figure}
\end{center}


\begin{lstlisting}[language=Python]
import tensorflow as tf
import matplotlib.pyplot as plt
import numpy as np
tf.set_random_seed(0)
np.random.seed(0)
x = np.linspace(-1,1,100).reshape(-1,1)
noise = np.random.normal(0,0.1,size=x.shape)
y = np.power(x,2)+noise
def gendata():
    t = np.linspace(-1,1,100).reshape(-1,1)
def save():
    print('This is save')
    tf_x = tf.placeholder(tf.float32,x.shape)
    tf_y = tf.placeholder(tf.float32,y.shape)
    l = tf.layers.dense(tf_x,10,tf.nn.relu)
    o = tf.layers.dense(l,1)
    loss = tf.losses.mean_squared_error(tf_y,o)
    train_op = tf.train.GradientDescentOptimizer(learning_rate=0.5).minimize(loss)
    sess = tf.Session()
    sess.run(tf.global_variables_initializer())
    saver = tf.train.Saver()
    for step in range(100):
        sess.run(train_op,{tf_x:x,tf_y:y})
    saver.save(sess,'params',write_meta_graph=False)
    pred,l = sess.run([o,loss],{tf_x:x,tf_y:y})
    plt.figure(1,figsize=(10,5))
    plt.subplot(121)
    plt.scatter(x,y)
    plt.plot(x,pred,'r-',lw=5)
    plt.text(-1,1.2,'save loss=%.4f'%l,fontdict={'size':15,'color':'red'})
def reload():
    print('This is reload')
    tf_x = tf.placeholder(tf.float32,x.shape)
    tf_y = tf.placeholder(tf.float32,y.shape)
    l_ = tf.layers.dense(tf_x,10,tf.nn.relu)
    o_ = tf.layers.dense(l_,1)
    loss_ = tf.losses.mean_squared_error(tf_y,o_)
    sess = tf.Session()
    saver = tf.train.Saver()
    saver.restore(sess,'params')
    pred,l = sess.run([o_,loss_],{tf_x:x,tf_y:y})
    plt.subplot(122)
    plt.scatter(x,y)
    plt.plot(x,pred,'r-',lw=5)
    plt.text(-1,1.2,'Reload Loss=%.4f'%l,fontdict={'size':15,'color':'red'})
    plt.show()
save()
tf.reset_default_graph()
reload()
\end{lstlisting}


\documentclass{book}
\usepackage[space]{ctex}
\usepackage{float}
\usepackage{hyperref}
\graphicspath{{./pic/chapter1/}{./pic/chapter2/}{./pic/chapter3/}{./pic/example/}}
\begin{document}
\chapter{扩展}
这个章节解释开发者如何增加功能到TensorFlow。
\section{TensorFlow架构}
我们设计TensorFlow是为了大规模分布式训练和推理，但是它也能灵活的支持一些新的机器学习模型实验和系统级别的优化。
这个文件描述了这个系统架构使得结合规模和灵活度成为可能。假设你熟悉TensorFlow基本的一些概念，像计算图，操作会话。
这个文档适合于那些想用当前API不支持的一些方法扩展TensorFlow，想要优化TensorFlow的硬件工程师，在大规模分布式系统上实现机器学习系统或者是任何想要了解TensorFlow的hood的人。读完它后你应该能读和修改TensorFlow核心代码。
\section{概述}
TensorFlow运行环境是一个跨平台的库，下图画出了常用的架构，C API分隔用户代码和核心代码。
\begin{center}
\begin{figure}[H]
\centering
\includegraphics{layers.png}
\caption{Tensorflow框架}
	\label{fig:tf_frame}
\end{figure}
\end{center}
\begin{itemize}
\item \textbf{Client}
\item 定义计算作为数据流图。
\item 用session初始化图。
\item Distributed Master
\item 从图中获取一个子图作为定义的参数给Session.run()
\item 将图分成多块在不同的进程和设备上运行。
\item 分配图块到worker service。
\item 用worker services初始化图块
\item \textbf{Worker services}
\item 调度图上的操作在可用的硬件平台(CPUs,GPUs)上执行。
\item 发送和接收worker service的操作结果。
\item \textbf{内核实现}。
\item 执行单个图操作的计算。
\end{itemize}
下图说明组件之间的交互。"job:worker/task:0"和"/job:ps/task:0"两个任务在wokers上。"PS"代表"parameter server":一个负责存储更新模型参数的任务。另一个任务优化参数时发送更新到这些参数，类似的在任务之间的分隔是不被要求的，但是它通常用于分配的训练。
\begin{figure}[H]
	\centering
	\includegraphics[scale=0.5]{diag1.jpg}
	\caption{交互}
	\label{fig:tf_ext2}
\end{figure}
注意Distributed Master和Worker Service仅仅存在于分布的TensorFlow。，单进程版本的TensorFlow包含一个特别的Session实现能做任何Distributed msdter能做的不仅仅是和本地进程通信。
下面的章节表述了TensorFlowlayer的核心。
\subsection{Client}
用户写TensorFlow程序构造计算图。这个程序既可以直接组成单个操作又可以用一个像Estimators API的方便的库组成神经网络乘和其它高级抽象。TensorFlow支持多种用户语言，但是我们优先使用Python和C++，仅仅是因为我们的内部用户熟悉它们。当特征被建立好后我们将它们接入C++。因此用户可以得到一个对所有语言优化的实现。大多数的训练库仅仅支持Python，但是C++支持更高效的推理。
用户创建一个会话，发送图的定义到ditributed master作为tf.GraphDef 协议缓冲区。然后客户评估图上的一个节点或者多个节点，评估触发一个distributed master的调用初始化计算。
在下图中，客户建立一个图，应用权重(w)到特征向量(x),增加偏置(b)保存结果。
\begin{figure}[H]
\centering
\includegraphics[scale=0.5]{graph_client.jpg}
	\caption{分割子图}
	\label{fig:tf_ext3}
\end{figure}
\subsection{Distributed master}
\begin{itemize}
\item 修剪图得到子图计算用户的节点请求。
\item 对于每一个加入的设备，分隔图获得子图。
\item 缓存这些块以至于它们能用在自序列中。
\end{itemize}
因为master查看每一步的计算，它用像常用的子表达式消除和常数折叠的标准的优化。它然后执行优化的子图。
下图显示一个可能的分隔。distributed master有组合的模型参数为了放置它们在参数服务器上。
这里图的边缘被分隔，distributed master发送接收节点在不同的任务间传送信息。
下面的distributed master传输子图到分布的任务。
\begin{figure}[H]
	\centering 
	\includegraphics[scale=0.5]{graph_master_cln.jpg}
	\caption{session处理}
	\label{fig:tf_ext4}
\end{figure}
下图5显示了我们例子图的一个可能分割。这个分布的master有组在一起的模型参数来防止他们在一个server上。
\begin{figure}[H]
	\centering 
	\includegraphics[scale=0.5]{graph_split1.jpg}
	\caption{图的分割}
	\label{fig:tf_ext5}
\end{figure}
这里图的边被分割，分布的master插入发送接受节点在分布的tasks传递信息
\begin{figure}[H]
	\centering 
	\includegraphics[scale=0.5]{graph_split2.jpg}
	\caption{消息传递}
	\label{fig:tf_ext6}
\end{figure}
这个分布式的master传递子图到分布的task
\begin{figure}[H]
	\centering 
	\includegraphics[scale=0.5]{graph_workers_cln.jpg}
	\caption{子图传递到分布的tasks}
	\label{fig:tf_ext7}
\end{figure}
\begin{itemize}
		\item \href{https://www.github.com/tensorflow/tensorflow/blob/r1.4/tensorflow/core/protobuf/master_service.proto}{MasterService API definition}
		\item \href{https://www.github.com/tensorflow/tensorflow/blob/r1.4/tensorflow/core/distributed_runtime/master_interface.h}{Master interface}
\end{itemize}
\subsection{Worker Service}
任务中的worker service。
\begin{itemize}
\item 处理master的请求。
\item 调度内核执行包含本地子图的操作
\item 任务间的直接通信。
\end{itemize}
我们优化worker service为了能用更小的花销以运行大的图。我们当前的实现能实现每秒执行上万张子图，使得大量的副本快速的训练。worker service布置内核到本地设备上然后通过利用多CPU多GPU尽可能的并行执行。
我们为源和目的设备对指定发送和接收操作。
\begin{itemize}
\item 用cudaMemcpyAsync() API在本地CPU和GPU之间转换，覆盖计算和数据的转化。
\item 用对等的DMA在不同的本地GPU之间转化避免通过主CPU的高昂代价。
\end{itemize}
对于任务间的转化，TensorFLow用多个协议，报错:
\begin{itemize}
\item gPRC over TCP
\item RDMA over COnverged Ethernet
\end{itemize}
我们对于NVIDIA的多GPU通信NCCL库有初步的支持,查看\href{https://www.github.com/tensorflow/tensorflow/blob/r1.4/tensorflow/contrib/nccl/python/ops/nccl_ops.py}{tf.contrib.nccl}
\section{内核实现}
运行环境包含超过200个标准操作包括数学，数组操作，控制流，状态管理操作。每一个操作对不同的设备有优化，一些操作内核用Eigen::Tensor实现，用C++模板生成在多核CPU和GPUs上生成高效的并行代码，然而我们优先用CuDNN这类更高效内核实现的库。我们也实现了\href{https://www.tensorflow.org/performance/quantization}{量化}，能在移动设备和高流通数据中心应用上更快地推理，用\href{https://github.com/google/gemmlowp}{gemmlowp}低精读矩阵库加速量化计算。
如果很难或者抵消的的表达子计算作为操作的组成，用户可以注册额外的进程通过C++提供更高效的实现，我们推荐你为一些重要的操作像ReLU和Sigmoid和相关的梯度注册你的融合内核，XLA编译器有一些尝试实现实现自动内核融合。
\section{添加一个新的操作}
如果你想创建一个TensorFlow库不存在的操作，我们推荐你首先用已经存在的Python操作或者函数写这个操作。如果不能，你可以创建一个自定义的C++
操作，这里有一些原因为什么你想创建一个自定义的C++操作
\begin{itemize}
		\item 它不容易或者可能表达你的操作作为一个存在的计算操作
		\item 它不能高效的表示你的操作为一个原生存在的组件
		\item 你想手动融合一个原始的组件将来编译器将发现很难融合
\end{itemize}
例如，假设你想实现一个像"median pooling",类似"MaxPool"操作，但是滑动窗上medians代替最大值。也许可以用一个操作组件做这个(例如用ExtractImagePatches和TopK)，但是也许作为一个本地操作性能不好或者存储低效。一如既往的首先尝试着表达你想用操作组件表达的是什么，如果证明真的比较困难或者低效仅仅选择添加一个新的操作。

为了结合你的操作，你需要:
\begin{enumerate}
		\item 在C++文件中注册一个新的操作，操作注册为操作的功能定义了一个接口，操作独立与op's的实现。例如，操作注册定义了操作的名字和操作的输入输出。它也定义和形状函数用于tensor形状的推理。
		\item 在C++实现操作。操作的实现成为内核，它是第一步祖册的具体实现。可以有多个核用于不同的输入输出类型或者架构(例如CPUs，GPUs)
		\item 创建一个Python包装(可选)，这个包装器是公共的API用于创建Python操作。一个默认的包装器从操作注册中生成，可以直接使用或者添加。
		\item 写一个函数计算操作的梯度(可选)
		\item 测试操作。我们通常为了方便在Python中做这个操作，但是你可以特使C++操作。如果你定义梯度，你可以用Python \href{https://www.tensorflow.org/api_docs/python/tf/test/compute_gradient_error}{gradient checker}验证他们.查看\href{https://www.github.com/tensorflow/tensorflow/blob/r1.4/tensorflow/python/kernel_tests/relu_op_test.py}{relu_op_test.py}作为一个例子测试relu类似操作的前馈函数和他们的梯度。
\end{enumerate}
\end{document}
